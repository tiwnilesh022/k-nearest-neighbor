{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN: \n",
    "- The KNN algorithm assumes that similar things exist in close proximity. \n",
    "- It's a supervised machine learning algorithm that can be used to solve both classification and regression problems. \n",
    "- KNN works on a similarity measure (eg. distance functions)\n",
    "- KNN is a non-parametric and lazy learning algorithm. \n",
    "    - Non-parametric means there is no assumption for underlying data distribution.\n",
    "    - Lazy algorithm means it does not need any training data points for model generation. All training data used in the testing phase.\n",
    "\n",
    "### Steps: The KNN Algorithm:\n",
    "- 1. Load the data.\n",
    "- 2. Initialize K to your chosen number of neighbors.\n",
    "- 3. For each query data point in the data.\n",
    "      - Calculate the distance between the query data point and the all other data points in the data.\n",
    "      - There are multiple distance functions like Euclidean distance, Hamming distance, Manhattan distance and Minkowski distance etc.\n",
    "      <img src = './Image/10.1 Image a.png' width=30% height=20%/>\n",
    "- 4. Sort the ordered collection of distances and indices from smallest to largest (in ascending order) by the distances.\n",
    "- 5. Pick the first K entries from the sorted collection.\n",
    "- 6. Get the labels of the selected K entries.\n",
    "- 7. If regression, return the mean of the K labels.\n",
    "- 8. If classification, return the mode of the K labels i.e, class of query data point is decided based on **majority voting**.\n",
    "<img src = './Image/10.1 Image b.gif' width=50% height=50%/>\n",
    "\n",
    "\n",
    "### Effect of K or n_neighbors:\n",
    "The number of neighbors is the core deciding factor.\n",
    "   - 1. As we decrease the value of K to 1, our predictions become less stable.\n",
    "   - 2. Inversely, as we increase the value of K, our predictions become more stable due to majority voting / averaging, and thus, more likely to make more accurate predictions (up to a certain point). Eventually, we begin to witness an increasing number of errors. It is at this point we know we have pushed the value of K too far.\n",
    "   - 3. In cases where we are taking a majority vote (e.g. picking the mode in a classification problem) among labels, we usually make K an odd number to have a tiebreaker.\n",
    "\n",
    "### How to choose the value of K?\n",
    "The number of neighbors(K) in KNN is a hyperparameter(controlling variable for the prediction model) that we need choose at the time of model building.\n",
    "   - 1) We can take the help of domain expert on the problem we are solving to get the best value of K.\n",
    "   - 2) We can use the cross validation to find best K. We try with different K values and check how the validation erroe rate is varying. We can choose point in the graph as best K."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "#### Pros:\n",
    "- KNN is a non parametric algorithm which means there are no assumptions to be met to implement KNN.\n",
    "- KNN doesn't explicitely build any model, it simply tags the new data entry based learning from historical data.\n",
    "- KNN can also be used for multiclass classification.\n",
    "- It is a instance-based learning- KNN is a memory based approach. The classifier immediately adapts as we collect new training data. It allows the algorithm to respond quickly to changes in the input during real-time use.\n",
    "\n",
    "#### Cons:\n",
    "- One of the biggest issue with KNN is to choose the optimal number of neighbors to be considered while classifying the new data entry.\n",
    "- As dataset grows efficiency or speed of algorithm declines very fast.\n",
    "- KNN works well with a small number of input features but as the number of features increases KNN struggles to predict the output of new data point. (called Curse of Dimensionality)\n",
    "- We need to have normalised data.\n",
    "- KNN doesn't perform well on imbalanced data.\n",
    "- KNN is very sensitive to outliers as it simply choose the neighbors based on distance criteria.\n",
    "- KNN inherently has no capability of dealing with missing value problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
